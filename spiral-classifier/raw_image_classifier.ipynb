{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "\n",
    "# import ML/DL libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import utils, svm, metrics # used to shuffle data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator # used for image augmentation\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "# used for building and training a new model\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications import VGG16, ResNet50\n",
    "\n",
    "# import functions from other python files\n",
    "from code_files.imagePreprocessing import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ********************************\n",
    "#           IMPORT IMAGES\n",
    "# ********************************\n",
    "# import images (and labels) and store in dataframe\n",
    "# FLAG: set the path to the desired dataset\n",
    "data_path = 'C:/Users/Rebecca/Documents/Virginia_Tech/Research/database-images/database-images/orig/spiral/'  \n",
    "\n",
    "trainImgs = pd.DataFrame()\n",
    "trainArray = []\n",
    "\n",
    "img_path = []\n",
    "lbl = []\n",
    "lblName = []\n",
    "\n",
    "for group in os.listdir(data_path): # group = healthy or parkinsons\n",
    "    for img in os.listdir(os.path.join(data_path, group)):\n",
    "        path = os.path.join(data_path, group, img)\n",
    "        img_path.append(path) \n",
    "\n",
    "        # convert the image and store as a matrix\n",
    "        drawing = cv2.imread(path)\n",
    "        drawing = cv2.resize(drawing, (224,224))\n",
    "\n",
    "        trainArray.append(drawing)\n",
    "\n",
    "        # store the labels\n",
    "        if group == 'healthy':\n",
    "            lbl.append(0)\n",
    "            lblName.append('healthy')\n",
    "        else:\n",
    "            lbl.append(1)\n",
    "            lblName.append('parkinsons')\n",
    "\n",
    "trainLbls = lbl\n",
    "trainImgs['image'] = img_path\n",
    "trainImgs['label'] = lblName\n",
    "\n",
    "\n",
    "# shuffle the data\n",
    "# trainImgs, trainArray, trainLbls = utils.shuffle(trainImgs, trainArray, trainLbls)\n",
    "\n",
    "# convert labels to categorical for training model\n",
    "trainLbls_categorical = tf.keras.utils.to_categorical(trainLbls)\n",
    "# print(\"Labels of first 5 images: \\n\", trainLbls_categorical[0:5])\n",
    "\n",
    "# used in the feature extraction section\n",
    "numImgs = len(trainLbls)\n",
    "print(\"Total number of images: \", numImgs)\n",
    "\n",
    "sns.countplot(trainLbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 conv1_pad (None, 230, 230, 3)\n",
      "2 conv1_conv (None, 112, 112, 64)\n",
      "3 conv1_bn (None, 112, 112, 64)\n",
      "4 conv1_relu (None, 112, 112, 64)\n",
      "7 conv2_block1_1_conv (None, 56, 56, 64)\n",
      "8 conv2_block1_1_bn (None, 56, 56, 64)\n",
      "9 conv2_block1_1_relu (None, 56, 56, 64)\n",
      "10 conv2_block1_2_conv (None, 56, 56, 64)\n",
      "11 conv2_block1_2_bn (None, 56, 56, 64)\n",
      "12 conv2_block1_2_relu (None, 56, 56, 64)\n",
      "13 conv2_block1_0_conv (None, 56, 56, 256)\n",
      "14 conv2_block1_3_conv (None, 56, 56, 256)\n",
      "15 conv2_block1_0_bn (None, 56, 56, 256)\n",
      "16 conv2_block1_3_bn (None, 56, 56, 256)\n",
      "17 conv2_block1_add (None, 56, 56, 256)\n",
      "18 conv2_block1_out (None, 56, 56, 256)\n",
      "19 conv2_block2_1_conv (None, 56, 56, 64)\n",
      "20 conv2_block2_1_bn (None, 56, 56, 64)\n",
      "21 conv2_block2_1_relu (None, 56, 56, 64)\n",
      "22 conv2_block2_2_conv (None, 56, 56, 64)\n",
      "23 conv2_block2_2_bn (None, 56, 56, 64)\n",
      "24 conv2_block2_2_relu (None, 56, 56, 64)\n",
      "25 conv2_block2_3_conv (None, 56, 56, 256)\n",
      "26 conv2_block2_3_bn (None, 56, 56, 256)\n",
      "27 conv2_block2_add (None, 56, 56, 256)\n",
      "28 conv2_block2_out (None, 56, 56, 256)\n",
      "29 conv2_block3_1_conv (None, 56, 56, 64)\n",
      "30 conv2_block3_1_bn (None, 56, 56, 64)\n",
      "31 conv2_block3_1_relu (None, 56, 56, 64)\n",
      "32 conv2_block3_2_conv (None, 56, 56, 64)\n",
      "33 conv2_block3_2_bn (None, 56, 56, 64)\n",
      "34 conv2_block3_2_relu (None, 56, 56, 64)\n",
      "35 conv2_block3_3_conv (None, 56, 56, 256)\n",
      "36 conv2_block3_3_bn (None, 56, 56, 256)\n",
      "37 conv2_block3_add (None, 56, 56, 256)\n",
      "38 conv2_block3_out (None, 56, 56, 256)\n",
      "39 conv3_block1_1_conv (None, 28, 28, 128)\n",
      "40 conv3_block1_1_bn (None, 28, 28, 128)\n",
      "41 conv3_block1_1_relu (None, 28, 28, 128)\n",
      "42 conv3_block1_2_conv (None, 28, 28, 128)\n",
      "43 conv3_block1_2_bn (None, 28, 28, 128)\n",
      "44 conv3_block1_2_relu (None, 28, 28, 128)\n",
      "45 conv3_block1_0_conv (None, 28, 28, 512)\n",
      "46 conv3_block1_3_conv (None, 28, 28, 512)\n",
      "47 conv3_block1_0_bn (None, 28, 28, 512)\n",
      "48 conv3_block1_3_bn (None, 28, 28, 512)\n",
      "49 conv3_block1_add (None, 28, 28, 512)\n",
      "50 conv3_block1_out (None, 28, 28, 512)\n",
      "51 conv3_block2_1_conv (None, 28, 28, 128)\n",
      "52 conv3_block2_1_bn (None, 28, 28, 128)\n",
      "53 conv3_block2_1_relu (None, 28, 28, 128)\n",
      "54 conv3_block2_2_conv (None, 28, 28, 128)\n",
      "55 conv3_block2_2_bn (None, 28, 28, 128)\n",
      "56 conv3_block2_2_relu (None, 28, 28, 128)\n",
      "57 conv3_block2_3_conv (None, 28, 28, 512)\n",
      "58 conv3_block2_3_bn (None, 28, 28, 512)\n",
      "59 conv3_block2_add (None, 28, 28, 512)\n",
      "60 conv3_block2_out (None, 28, 28, 512)\n",
      "61 conv3_block3_1_conv (None, 28, 28, 128)\n",
      "62 conv3_block3_1_bn (None, 28, 28, 128)\n",
      "63 conv3_block3_1_relu (None, 28, 28, 128)\n",
      "64 conv3_block3_2_conv (None, 28, 28, 128)\n",
      "65 conv3_block3_2_bn (None, 28, 28, 128)\n",
      "66 conv3_block3_2_relu (None, 28, 28, 128)\n",
      "67 conv3_block3_3_conv (None, 28, 28, 512)\n",
      "68 conv3_block3_3_bn (None, 28, 28, 512)\n",
      "69 conv3_block3_add (None, 28, 28, 512)\n",
      "70 conv3_block3_out (None, 28, 28, 512)\n",
      "71 conv3_block4_1_conv (None, 28, 28, 128)\n",
      "72 conv3_block4_1_bn (None, 28, 28, 128)\n",
      "73 conv3_block4_1_relu (None, 28, 28, 128)\n",
      "74 conv3_block4_2_conv (None, 28, 28, 128)\n",
      "75 conv3_block4_2_bn (None, 28, 28, 128)\n",
      "76 conv3_block4_2_relu (None, 28, 28, 128)\n",
      "77 conv3_block4_3_conv (None, 28, 28, 512)\n",
      "78 conv3_block4_3_bn (None, 28, 28, 512)\n",
      "79 conv3_block4_add (None, 28, 28, 512)\n",
      "80 conv3_block4_out (None, 28, 28, 512)\n",
      "81 conv4_block1_1_conv (None, 14, 14, 256)\n",
      "82 conv4_block1_1_bn (None, 14, 14, 256)\n",
      "83 conv4_block1_1_relu (None, 14, 14, 256)\n",
      "84 conv4_block1_2_conv (None, 14, 14, 256)\n",
      "85 conv4_block1_2_bn (None, 14, 14, 256)\n",
      "86 conv4_block1_2_relu (None, 14, 14, 256)\n",
      "87 conv4_block1_0_conv (None, 14, 14, 1024)\n",
      "88 conv4_block1_3_conv (None, 14, 14, 1024)\n",
      "89 conv4_block1_0_bn (None, 14, 14, 1024)\n",
      "90 conv4_block1_3_bn (None, 14, 14, 1024)\n",
      "91 conv4_block1_add (None, 14, 14, 1024)\n",
      "92 conv4_block1_out (None, 14, 14, 1024)\n",
      "93 conv4_block2_1_conv (None, 14, 14, 256)\n",
      "94 conv4_block2_1_bn (None, 14, 14, 256)\n",
      "95 conv4_block2_1_relu (None, 14, 14, 256)\n",
      "96 conv4_block2_2_conv (None, 14, 14, 256)\n",
      "97 conv4_block2_2_bn (None, 14, 14, 256)\n",
      "98 conv4_block2_2_relu (None, 14, 14, 256)\n",
      "99 conv4_block2_3_conv (None, 14, 14, 1024)\n",
      "100 conv4_block2_3_bn (None, 14, 14, 1024)\n",
      "101 conv4_block2_add (None, 14, 14, 1024)\n",
      "102 conv4_block2_out (None, 14, 14, 1024)\n",
      "103 conv4_block3_1_conv (None, 14, 14, 256)\n",
      "104 conv4_block3_1_bn (None, 14, 14, 256)\n",
      "105 conv4_block3_1_relu (None, 14, 14, 256)\n",
      "106 conv4_block3_2_conv (None, 14, 14, 256)\n",
      "107 conv4_block3_2_bn (None, 14, 14, 256)\n",
      "108 conv4_block3_2_relu (None, 14, 14, 256)\n",
      "109 conv4_block3_3_conv (None, 14, 14, 1024)\n",
      "110 conv4_block3_3_bn (None, 14, 14, 1024)\n",
      "111 conv4_block3_add (None, 14, 14, 1024)\n",
      "112 conv4_block3_out (None, 14, 14, 1024)\n",
      "113 conv4_block4_1_conv (None, 14, 14, 256)\n",
      "114 conv4_block4_1_bn (None, 14, 14, 256)\n",
      "115 conv4_block4_1_relu (None, 14, 14, 256)\n",
      "116 conv4_block4_2_conv (None, 14, 14, 256)\n",
      "117 conv4_block4_2_bn (None, 14, 14, 256)\n",
      "118 conv4_block4_2_relu (None, 14, 14, 256)\n",
      "119 conv4_block4_3_conv (None, 14, 14, 1024)\n",
      "120 conv4_block4_3_bn (None, 14, 14, 1024)\n",
      "121 conv4_block4_add (None, 14, 14, 1024)\n",
      "122 conv4_block4_out (None, 14, 14, 1024)\n",
      "123 conv4_block5_1_conv (None, 14, 14, 256)\n",
      "124 conv4_block5_1_bn (None, 14, 14, 256)\n",
      "125 conv4_block5_1_relu (None, 14, 14, 256)\n",
      "126 conv4_block5_2_conv (None, 14, 14, 256)\n",
      "127 conv4_block5_2_bn (None, 14, 14, 256)\n",
      "128 conv4_block5_2_relu (None, 14, 14, 256)\n",
      "129 conv4_block5_3_conv (None, 14, 14, 1024)\n",
      "130 conv4_block5_3_bn (None, 14, 14, 1024)\n",
      "131 conv4_block5_add (None, 14, 14, 1024)\n",
      "132 conv4_block5_out (None, 14, 14, 1024)\n",
      "133 conv4_block6_1_conv (None, 14, 14, 256)\n",
      "134 conv4_block6_1_bn (None, 14, 14, 256)\n",
      "135 conv4_block6_1_relu (None, 14, 14, 256)\n",
      "136 conv4_block6_2_conv (None, 14, 14, 256)\n",
      "137 conv4_block6_2_bn (None, 14, 14, 256)\n",
      "138 conv4_block6_2_relu (None, 14, 14, 256)\n",
      "139 conv4_block6_3_conv (None, 14, 14, 1024)\n",
      "140 conv4_block6_3_bn (None, 14, 14, 1024)\n",
      "141 conv4_block6_add (None, 14, 14, 1024)\n",
      "142 conv4_block6_out (None, 14, 14, 1024)\n",
      "143 conv5_block1_1_conv (None, 7, 7, 512)\n",
      "144 conv5_block1_1_bn (None, 7, 7, 512)\n",
      "145 conv5_block1_1_relu (None, 7, 7, 512)\n",
      "146 conv5_block1_2_conv (None, 7, 7, 512)\n",
      "147 conv5_block1_2_bn (None, 7, 7, 512)\n",
      "148 conv5_block1_2_relu (None, 7, 7, 512)\n",
      "149 conv5_block1_0_conv (None, 7, 7, 2048)\n",
      "150 conv5_block1_3_conv (None, 7, 7, 2048)\n",
      "151 conv5_block1_0_bn (None, 7, 7, 2048)\n",
      "152 conv5_block1_3_bn (None, 7, 7, 2048)\n",
      "153 conv5_block1_add (None, 7, 7, 2048)\n",
      "154 conv5_block1_out (None, 7, 7, 2048)\n",
      "155 conv5_block2_1_conv (None, 7, 7, 512)\n",
      "156 conv5_block2_1_bn (None, 7, 7, 512)\n",
      "157 conv5_block2_1_relu (None, 7, 7, 512)\n",
      "158 conv5_block2_2_conv (None, 7, 7, 512)\n",
      "159 conv5_block2_2_bn (None, 7, 7, 512)\n",
      "160 conv5_block2_2_relu (None, 7, 7, 512)\n",
      "161 conv5_block2_3_conv (None, 7, 7, 2048)\n",
      "162 conv5_block2_3_bn (None, 7, 7, 2048)\n",
      "163 conv5_block2_add (None, 7, 7, 2048)\n",
      "164 conv5_block2_out (None, 7, 7, 2048)\n",
      "165 conv5_block3_1_conv (None, 7, 7, 512)\n",
      "166 conv5_block3_1_bn (None, 7, 7, 512)\n",
      "167 conv5_block3_1_relu (None, 7, 7, 512)\n",
      "168 conv5_block3_2_conv (None, 7, 7, 512)\n",
      "169 conv5_block3_2_bn (None, 7, 7, 512)\n",
      "170 conv5_block3_2_relu (None, 7, 7, 512)\n",
      "171 conv5_block3_3_conv (None, 7, 7, 2048)\n",
      "172 conv5_block3_3_bn (None, 7, 7, 2048)\n",
      "173 conv5_block3_add (None, 7, 7, 2048)\n",
      "174 conv5_block3_out (None, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "# ******************************\n",
    "#         IMPORT MODEL\n",
    "# ******************************\n",
    "# import ResNet-50 pretrained model \n",
    "model = ResNet50(weights='imagenet', include_top=False,input_shape=(224,224,3)) # setting include_top=False removes the fully connected layers of the model\n",
    "# model.summary()\n",
    "\n",
    "# summarize feature map shapes # FLAG: can uncomment for feature visualization\n",
    "for i in range(len(model.layers)):\n",
    "    layer=model.layers[i]\n",
    "    # check for conv layer\n",
    "    if 'conv' in layer.name:\n",
    "        print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 277ms/step\n",
      "Image_0_block_4_vgg16_orig_HT.png\n",
      "Image_0_block_17_vgg16_orig_HT.png\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Image_1_block_4_vgg16_orig_HT.png\n",
      "Image_1_block_17_vgg16_orig_HT.png\n"
     ]
    }
   ],
   "source": [
    "# ******************************\n",
    "#     VISUALIZE FEATURE MAPS\n",
    "# ******************************\n",
    "# choose second conv block from each layer to display\n",
    "blocks = [4, 17]\n",
    "output_layers = [model.layers[i].output for i in blocks]\n",
    "# redefine model to output right after each conv layer\n",
    "vis_model = Model(inputs=model.inputs, outputs=output_layers)\n",
    "\n",
    "train_feat, test_feat, train_lbls, test_lbls = train_test_split(trainArray, trainLbls, test_size=0.2, random_state=42)\n",
    "\n",
    "# select images to save visualizations for and put them in an array\n",
    "# manually choose two healthy and two parkinsons\n",
    "img2vis = np.array([test_feat[0], test_feat[1]])\n",
    "\n",
    "# iterate through each image and save the feature maps\n",
    "for i in range(len(img2vis)):\n",
    "    img = np.expand_dims(img2vis[i], axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    feature_maps = vis_model.predict(img)\n",
    "    blocknum = 0\n",
    "\n",
    "    for fmap in feature_maps:\n",
    "        fmap_size = np.shape(fmap)\n",
    "        # determine the number of images to plot\n",
    "        if fmap_size[3]==64:\n",
    "            rows=8;cols=8\n",
    "        elif fmap_size[3]==128:\n",
    "            rows=16; cols=8\n",
    "        elif fmap_size[3]==256:\n",
    "            rows=16;cols=16\n",
    "        else:\n",
    "            rows=16;cols=32\n",
    "\n",
    "        itr = 1\n",
    "        fig, ax = plt.subplots(rows,cols, figsize=(75,75))\n",
    "        title = 'Image_' + str(i) + 'fmap: ' + str(np.shape(fmap))\n",
    "        fig.suptitle(title)\n",
    "\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                ax[r][c].imshow(fmap[0,:,:,itr-1], cmap='gray')\n",
    "                itr += 1\n",
    "\n",
    "        plt.tight_layout()\n",
    "        savename = 'Image_' + str(i) + '_block_' +  str(blocks[blocknum]) + '_vgg16_orig_HT.png'\n",
    "        print(savename)\n",
    "        fig.savefig(savename)\n",
    "        plt.close()\n",
    "        blocknum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 480 validated image filenames belonging to 2 classes.\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "# ******************************************************************************************************\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "#                                           CLASSIFICATION  \n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# ******************************************************************************************************\n",
    "\n",
    "# 1. VGG16 or RestNet50\n",
    "# 2. SVM\n",
    "# 3. Naive Bayes\n",
    "# 4. Random Forest\n",
    "\n",
    "# .........................\n",
    "#    FEATURE EXTRACTION\n",
    "# .........................\n",
    "# define a function that will extract the features from conv network\n",
    "def extract_features(imgs, num_imgs):\n",
    "    datagen = ImageDataGenerator(rescale=1./255) # define to rescale pixels in image\n",
    "    batch_size = 32\n",
    "    \n",
    "    features = np.zeros(shape=(num_imgs, 7,7,2048)) # shape equal to output of convolutional base \n",
    "    lbls = np.zeros(shape=(num_imgs,2))\n",
    "\n",
    "    # preprocess data\n",
    "    generator = datagen.flow_from_dataframe(imgs, x_col = 'image', y_col='label', target_size=(224,224), class_mode='categorical', batch_size=batch_size)\n",
    "\n",
    "    # Pass data through convolutional base\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = model.predict(inputs_batch)\n",
    "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
    "        lbls[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= num_imgs:\n",
    "            break\n",
    "    return features, lbls\n",
    "\n",
    "# extract features for both the trainImgs and testImgs\n",
    "train_feat, train_lbls = extract_features(trainImgs, numImgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================\n",
    "#       Train on Fully Connected Layers\n",
    "# =============================\n",
    "# split into training and testing data\n",
    "train_feat, test_feat, train_lbls, test_lbls = train_test_split(train_feat, train_lbls, test_size=0.2, random_state=10, stratify=train_lbls)\n",
    "trainArray, testArray, _,_ = train_test_split(trainArray, trainLbls, test_size=0.2, random_state=10, stratify=trainLbls)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,3))\n",
    "sns.countplot(train_lbls[:,1], ax=ax[0])\n",
    "sns.countplot(test_lbls[:,1], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  1\n",
      "Loss:  0.5724338293075562 | Accuracy:  0.6233766078948975\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001998B879E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Random State:  2\n",
      "Loss:  0.5165125727653503 | Accuracy:  0.7402597665786743\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  3\n",
      "Loss:  0.48370540142059326 | Accuracy:  0.8181818127632141\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  4\n",
      "Loss:  0.5244485139846802 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  5\n",
      "Loss:  0.6034515500068665 | Accuracy:  0.6623376607894897\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  6\n",
      "Loss:  0.553978681564331 | Accuracy:  0.7142857313156128\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  7\n",
      "Loss:  0.5052992701530457 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  8\n",
      "Loss:  0.5432925820350647 | Accuracy:  0.7662337422370911\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  9\n",
      "Loss:  0.4988749027252197 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  10\n",
      "Loss:  0.4864385426044464 | Accuracy:  0.8051947951316833\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Random State:  11\n",
      "Loss:  0.49782800674438477 | Accuracy:  0.8051947951316833\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  12\n",
      "Loss:  0.5382084250450134 | Accuracy:  0.7792207598686218\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  13\n",
      "Loss:  0.5254298448562622 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  14\n",
      "Loss:  0.4908939599990845 | Accuracy:  0.7662337422370911\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  15\n",
      "Loss:  0.48624053597450256 | Accuracy:  0.8441558480262756\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  16\n",
      "Loss:  0.5885079503059387 | Accuracy:  0.6233766078948975\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  17\n",
      "Loss:  0.4654333293437958 | Accuracy:  0.8311688303947449\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  18\n",
      "Loss:  0.5357010364532471 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  19\n",
      "Loss:  0.5751781463623047 | Accuracy:  0.7142857313156128\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  20\n",
      "Loss:  0.4824518859386444 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  21\n",
      "Loss:  0.5755520462989807 | Accuracy:  0.7402597665786743\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Random State:  22\n",
      "Loss:  0.48902106285095215 | Accuracy:  0.7792207598686218\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  23\n",
      "Loss:  0.5436789393424988 | Accuracy:  0.7532467246055603\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Random State:  24\n",
      "Loss:  0.5182079672813416 | Accuracy:  0.7662337422370911\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Random State:  25\n",
      "Loss:  0.5316852927207947 | Accuracy:  0.7922077775001526\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Average accuracy:  0.7537662220001221\n",
      "Standard deviation:  0.05392097591693965\n",
      "Average TNR:  0.7658679701687272\n",
      "Average TPR:  0.749734237150202\n"
     ]
    }
   ],
   "source": [
    "# train pre-trained features\n",
    "# evaluate on VGG16 classifier (using cross validation)\n",
    "# define a function that will fit the model\n",
    "def defineModel(size): # size is the dimension of the last layer in the pretrained model\n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D(input_shape=(size,size,2048)))\n",
    "    # global average pooling is used instead of fully connected layers on top of the feature maps\n",
    "    # it takes the average of each feature map and the resulting layer is fed directly into the softmax layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # model.summary()\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-3)  # use the Adam optimizer and set an effective learning rate \n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# train the model using cross validation\n",
    "# will start with k-fold cross validation, taking 80% as training each fold\n",
    "\n",
    "def fit_and_evaluate(train_feat, train_lbls, val_feat, val_lbls, epochs):\n",
    "    model = None\n",
    "    model = defineModel(7) # FLAG: need to set the size based on the last layer\n",
    "    trained_model = model.fit(train_feat, train_lbls, batch_size=32, epochs=epochs, validation_data=(val_feat, val_lbls), callbacks=model_chkpt, verbose=0)\n",
    "\n",
    "    # testScore = model.evaluate(test_feat, test_lbls)\n",
    "    return trained_model\n",
    "\n",
    "def importModel(filename, testAug, testAugLabel):\n",
    "    modelPath = filename\n",
    "    testModel = tf.keras.models.load_model(modelPath)\n",
    "\n",
    "    loss, acc = testModel.evaluate(np.array(testAug), testAugLabel, verbose=0)\n",
    "    print(\"Loss: \", loss, \"| Accuracy: \", acc)\n",
    "\n",
    "    # classification report\n",
    "    pred = testModel.predict(np.array(testAug))\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    label_pred = np.argmax(testAugLabel, axis=1)\n",
    "    # print(classification_report(label_pred, pred))\n",
    "\n",
    "    # confusion matrix\n",
    "    cfm = confusion_matrix(label_pred, pred)\n",
    "    # print(cfm)\n",
    "\n",
    "    tpr = cfm[0][0] / (cfm[0][0] + cfm[1][0])\n",
    "    tnr = cfm[1][1] / (cfm[1][1] + cfm[0][1])\n",
    "\n",
    "    return acc, tpr, tnr, cfm\n",
    "\n",
    "# train with k-fold validation\n",
    "model_history = []\n",
    "epochs = 250\n",
    "\n",
    "num_val_samples = int(np.ceil(len(trainArray) * 0.20))\n",
    "k = int(np.floor(len(trainArray) / num_val_samples))\n",
    "\n",
    "fcl_acc = []\n",
    "fcl_tpr = []\n",
    "fcl_tnr = []\n",
    "fcl_cfm = []\n",
    "\n",
    "for i in range(25):\n",
    "    print('Random State: ', i+1)\n",
    "    rs = np.linspace(1, 25, 25).astype(np.int)\n",
    "\n",
    "    # define model checkpoint callback\n",
    "    ckpt_name = '20221219_folador_' + str(i+1)+ '.h5'\n",
    "    model_chkpt = tf.keras.callbacks.ModelCheckpoint(ckpt_name, verbose=0, save_best_only=True)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_feat, train_lbls, test_size=0.2, random_state=rs[i], stratify=train_lbls)\n",
    "\n",
    "    for j in range(k):\n",
    "        print(\"Training on fold K = \", j+1, end='\\r')\n",
    "        startPt = j * num_val_samples\n",
    "        endPt   = (j+1) * num_val_samples\n",
    "\n",
    "        if endPt > len(x_train):\n",
    "            endPt = len(x_train)\n",
    "\n",
    "        val_x = x_train[startPt:endPt]\n",
    "        val_y = y_train[startPt:endPt]\n",
    "        train_x = np.delete(x_train, np.linspace(startPt, endPt-1, num_val_samples).astype(np.int), axis=0)\n",
    "        train_y = np.delete(y_train, np.linspace(startPt, endPt-1, num_val_samples).astype(np.int), axis=0)\n",
    "\n",
    "        model_history.append(fit_and_evaluate(train_x, train_y, val_x, val_y, epochs=epochs))\n",
    "        # print(model_history)\n",
    "\n",
    "\n",
    "    temp_acc, temp_tpr, temp_tnr, temp_cfm = importModel(ckpt_name, x_test, y_test)\n",
    "\n",
    "    fcl_acc.append(temp_acc)\n",
    "    fcl_tpr.append(temp_tpr)\n",
    "    fcl_tnr.append(temp_tnr)\n",
    "    fcl_cfm.append(temp_cfm)\n",
    "\n",
    "print(\"Average accuracy: \", np.mean(fcl_acc))\n",
    "print(\"Standard deviation: \", np.std(fcl_acc))\n",
    "print(\"Average TNR: \", np.mean(fcl_tnr))\n",
    "print(\"Average TPR: \", np.mean(fcl_tpr))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[29, 10],\n",
      "       [19, 19]], dtype=int64), array([[29,  9],\n",
      "       [11, 28]], dtype=int64), array([[28, 10],\n",
      "       [ 4, 35]], dtype=int64), array([[32,  6],\n",
      "       [13, 26]], dtype=int64), array([[33,  6],\n",
      "       [20, 18]], dtype=int64), array([[27, 11],\n",
      "       [11, 28]], dtype=int64), array([[27, 12],\n",
      "       [ 7, 31]], dtype=int64), array([[34,  5],\n",
      "       [13, 25]], dtype=int64), array([[28, 10],\n",
      "       [ 9, 30]], dtype=int64), array([[32,  7],\n",
      "       [ 8, 30]], dtype=int64), array([[30,  9],\n",
      "       [ 6, 32]], dtype=int64), array([[30,  9],\n",
      "       [ 8, 30]], dtype=int64), array([[32,  6],\n",
      "       [13, 26]], dtype=int64), array([[28, 11],\n",
      "       [ 7, 31]], dtype=int64), array([[32,  6],\n",
      "       [ 6, 33]], dtype=int64), array([[28, 11],\n",
      "       [18, 20]], dtype=int64), array([[32,  7],\n",
      "       [ 6, 32]], dtype=int64), array([[28, 10],\n",
      "       [ 9, 30]], dtype=int64), array([[30,  9],\n",
      "       [13, 25]], dtype=int64), array([[31,  8],\n",
      "       [11, 27]], dtype=int64), array([[28, 11],\n",
      "       [ 9, 29]], dtype=int64), array([[30,  9],\n",
      "       [ 8, 30]], dtype=int64), array([[28, 11],\n",
      "       [ 8, 30]], dtype=int64), array([[32,  6],\n",
      "       [12, 27]], dtype=int64), array([[33,  5],\n",
      "       [11, 28]], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "print(fcl_cfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 0s - loss: 0.3884 - accuracy: 0.8438 - 106ms/epoch - 35ms/step\n",
      "Loss:  0.3883879482746124 | Accuracy:  0.84375\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85        48\n",
      "           1       0.85      0.83      0.84        48\n",
      "\n",
      "    accuracy                           0.84        96\n",
      "   macro avg       0.84      0.84      0.84        96\n",
      "weighted avg       0.84      0.84      0.84        96\n",
      "\n",
      "[[41  7]\n",
      " [ 8 40]]\n"
     ]
    }
   ],
   "source": [
    "# ...............\n",
    "# VIEW RESULTS\n",
    "# ...............   \n",
    "\n",
    "new_model = False # FLAG\n",
    "\n",
    "num_val_samples = int(np.ceil(len(trainArray) * 0.20))\n",
    "k = int(np.floor(len(trainArray) / num_val_samples))\n",
    "\n",
    "if new_model==True:\n",
    "    # plot the accuracy and loss functions for each fold\n",
    "    color = ['blue', 'black', 'red', 'green','orange', 'cyan', 'grey', 'yellow', 'fuchsia']\n",
    "    f, ax = plt.subplots(2, k, figsize=(35,6))\n",
    "    for i in range(k):\n",
    "        ax[0][i].plot(model_history[i].history['accuracy'], label='train acc', color=color[i])\n",
    "        ax[0][i].plot(model_history[i].history['val_accuracy'], label='val acc', linestyle= ':', color=color[i])\n",
    "        ax[0][i].axis([-10,epochs, .2, 1.1])\n",
    "        ax[0][i].legend()\n",
    "\n",
    "        subplot_title = 'k = ' + str(i+1)\n",
    "        ax[0][i].title.set_text(subplot_title)\n",
    "\n",
    "    for i in range(k):\n",
    "        ax[1][i].plot(model_history[i].history['loss'], label='train loss', color=color[i])\n",
    "        ax[1][i].plot(model_history[i].history['val_loss'], label='val loss', linestyle= ':', color=color[i])\n",
    "        ax[1][i].axis([-10,epochs, .0, 1.1])\n",
    "        ax[1][i].legend()\n",
    "\n",
    "# ---------------------------------\n",
    "#   LOAD PRE-EXISTING MODEL MODEL\n",
    "# ---------------------------------\n",
    "def importModel(filename, testAug, testAugLabel):\n",
    "    modelPath = 'savedModels/' + filename\n",
    "    testModel = tf.keras.models.load_model(modelPath)\n",
    "\n",
    "    loss, acc = testModel.evaluate(np.array(testAug), testAugLabel, verbose=2)\n",
    "    print(\"Loss: \", loss, \"| Accuracy: \", acc)\n",
    "\n",
    "    # classification report\n",
    "    pred = testModel.predict(np.array(testAug))\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    label_pred = np.argmax(testAugLabel, axis=1)\n",
    "    print(classification_report(label_pred, pred))\n",
    "\n",
    "    # confusion matrix\n",
    "    cmat = confusion_matrix(label_pred, pred)\n",
    "    print(cmat)\n",
    "\n",
    "\n",
    "    return testModel\n",
    "\n",
    "# load existing model and evaluate the test data\n",
    "testmodel = importModel('20221219_kfold_folador_orig_bs32_rs10.h5', test_feat, test_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e234cb3a6d44168381e6f6673d50d0e52fa65a6fba8cecddf6f45db9097571d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
