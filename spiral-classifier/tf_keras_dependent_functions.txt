# from keras.preprocessing.image import ImageDataGenerator
# import tensorflow as tf

##### Image preprocessing

# function to split data into train, validation, and test
def splitData(df):
    # randomly split data in train and validation subsets (70-30 split)
    # stratify attempts to keep the labels 50-50 in the validation data (i.e. 7 total 0's and 8 total 1's)
    train_feature, val_feature, train_label, val_label = train_test_split(df['images'], df['label'], test_size=0.30, stratify=df['label'])

    # shuffle data
    train_feature, train_label = utils.shuffle(train_feature, train_label)
    val_feature, val_label = utils.shuffle(val_feature, val_label)

    ## (OPTIONAL) split validation data into validation and testing data
    val_feature, test_feature, val_label, test_label = train_test_split(val_feature, val_label, test_size=0.5, shuffle=False)

    # sort the test array so that all healthy images are first and PD images are last
    # this is useful for later when plotting
    testDF = pd.DataFrame()
    testDF['img'] = test_feature
    testDF['lbl'] = test_label
    testDF = testDF.sort_values('lbl')

    test_feature = testDF['img']
    test_label   = testDF['lbl']

    print("total validation samples: ", len(val_label))
    print("total testing samples: ", len(test_label))
    print('total training samples: ', len(train_label))

    return train_feature, train_label, val_feature, val_label, test_feature, test_label
# --------------------------------------------------------------------------------------------------

# function to perform data augmentation to have more data
def imgAug(train_array, train_label, val_array, val_label, test_array, test_label):
    ## -------------------------------------------------------------------
    #       Artificially create more images for a bigger dataset
    ## -------------------------------------------------------------------
    # define functions to generate batches of data containing augmented images
    # use for training data only
    train_gen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=180,
        fill_mode='nearest',
        brightness_range=[.4,1.4],
        vertical_flip = True,
        horizontal_flip = True
    )

    # use for validation and testing data (OPTIONAL: can make this different than training)
    test_gen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=180,
        fill_mode='nearest',
        brightness_range=[.4,1.4],
        vertical_flip = True,
        horizontal_flip = True
    )

    ## define function to artificially add more training, validation, and testing images
    # takes as inputs the dataset array, dataset labels, and the number of additional augmented images per each original image
    def generateAdditionalData(dataset_array, dataset_label, numImgs):
        dataAug = []
        dataAugLabel = []

        # iterate through each image in the data_array and create more images with the features specified by ImageDataGenerator
        for (idx,Lbl) in enumerate(dataset_label):
            tempImg = np.expand_dims(dataset_array[idx], axis=0) # use for grayscale
            # tempImg = train_array[idx]                         # use for rgb
            aug = train_gen.flow(tempImg, batch_size=1, shuffle=True)
            for addImages in range(numImgs):
                augImg = next(aug)[0] #.astype('uint8')
                if np.size(augImg) == 128**2:
                    dataAug.append(augImg)
                    dataAugLabel.append(Lbl)

        return dataAug, dataAugLabel

    trainAug, trainAugLabel = generateAdditionalData(train_array, train_label, 90)
    valAug, valAugLabel     = generateAdditionalData(val_array, val_label, 90)
    testAug, testAugLabel   = generateAdditionalData(test_array, test_label, 90)

    # covert label array to binary class matrix (healthy, PD)
    trainAugLabel = tf.keras.utils.to_categorical(np.array(trainAugLabel))
    valAugLabel = tf.keras.utils.to_categorical(np.array(valAugLabel))
    testAugLabel = tf.keras.utils.to_categorical(np.array(testAugLabel))

    # shuffle data one last time
    trainAug, trainAugLabel = utils.shuffle(trainAug, trainAugLabel)
    valAug, valAugLabel = utils.shuffle(valAug, valAugLabel)
    testAug, testAugLabel = utils.shuffle(testAug, testAugLabel)

    return trainAug, trainAugLabel, valAug, valAugLabel, testAug, testAugLabel
# --------------------------------------------------------------------------------------------------

##### CNN Models

# import libraries
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# build and train a new model
def trainModel(trainAug, trainAugLabel, valAug, valAugLabel, modelName=None):
    reg = tf.keras.regularizers.l2(0.001)               # include a regularizer to help prevent overfitting
    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)  # use the Adam optimizer and set an effective learning rate 

    # build a model
    model = Sequential([
        Conv2D(32, (3,3), padding='same', strides=(1,1), dilation_rate = 1, activation='relu', kernel_regularizer=reg, input_shape=(128,128,1)),
        MaxPool2D((3,3), strides=(1,1)),
        Conv2D(32, (3,3), padding='same', strides=(2,2), dilation_rate = 1, activation='relu', kernel_regularizer=reg),
        MaxPool2D((5,5), strides=(1,1)),
        Conv2D(64, (5,5), padding='same', strides=(1,1), dilation_rate = 2, activation='relu', kernel_regularizer=reg),
        MaxPool2D((5,5), strides=(2,2)),
        Conv2D(128, (7,7), padding='same', strides=(2,2), dilation_rate = 1, activation='relu', kernel_regularizer=reg),
        MaxPool2D((5,5), strides=(1,1)),
        Flatten(),

        ## include some fully connected layers
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(2,activation='softmax') # softmax used for classification, sigmoid better for regression
    ])

    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    ## Train the model
    trained_model = model.fit(np.array(trainAug), trainAugLabel, batch_size=128, epochs=35, validation_data=(np.array(valAug), valAugLabel))

    if modelName!=None:
        # save model
        savenameh5 = modelName + '.h5'
        savenametf = modelName + '.tf'
        model.save(savenameh5)
        model.save(savenametf)

    # -------------------------------------
    # plot and save the results
    # Accuracy and Validation Accuracy
    accuracy1 = trained_model.history['accuracy']
    val_acc1 = trained_model.history['val_accuracy']
    epochs = range(len(accuracy1))

    fig, ax = plt.subplots(1,2)
    fig.set_size_inches(18,8)
    ax[0].plot(epochs, accuracy1, 'b', label='Training Accuracy')
    ax[0].plot(epochs, val_acc1, 'r', label='Validation Accuracy')
    ax[0].title.set_text('Accuracy Graph')
    ax[0].legend()
    ax[0].grid()

    # Loss and Validation Loss
    loss1 = trained_model.history['loss']
    val_loss1 = trained_model.history['val_loss']

    ax[1].plot(epochs, loss1, 'b', label='Training Loss')
    ax[1].plot(epochs, val_loss1, 'r', label='Validation Loss')
    ax[1].title.set_text('Loss Graph')
    ax[1].legend()
    ax[1].grid()

    savefigName = modelName + 'accuracy_loss_graph.png'
    ax.savefig(savefigName)


# import a pre-existing model to run on test data (takes h5 model as input)
def importModel(filename, testAug, testAugLabel):
    modelPath = 'savedModels/saved_h5_models/' + filename
    testModel = tf.keras.models.load_model(modelPath)

    loss, acc = testModel.evaluate(np.array(testAug), testAugLabel, verbose=2)
    print("Loss: ", loss, "| Accuracy: ", acc)

    return testModel

##### 